<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>GitLab MCP Server tools 功能扩展实战 - 小马哥的博客</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="马景贺（小马哥）"><meta name=description content="使用 typescript 扩展 GitLab MCP Server tools"><meta name=keywords content="Hugo,theme,even"><meta name=generator content="Hugo 0.79.1 with theme even"><link rel=canonical href=https://majinghe.github.io/aigc/llm-work/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.c7bc1becf36bcf6a9ebd25d2947e43a2eb745ddb0c9a32b43126fd7fa460c351.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="GitLab MCP Server tools 功能扩展实战"><meta property="og:description" content="使用 typescript 扩展 GitLab MCP Server tools"><meta property="og:type" content="article"><meta property="og:url" content="https://majinghe.github.io/aigc/llm-work/"><meta property="article:published_time" content="2025-05-01T10:05:42+08:00"><meta property="article:modified_time" content="2025-05-01T10:05:42+08:00"><meta itemprop=name content="GitLab MCP Server tools 功能扩展实战"><meta itemprop=description content="使用 typescript 扩展 GitLab MCP Server tools"><meta itemprop=datePublished content="2025-05-01T10:05:42+08:00"><meta itemprop=dateModified content="2025-05-01T10:05:42+08:00"><meta itemprop=wordCount content="2093"><meta itemprop=keywords content="AI,MCP,GitLab,TypeScript,"><meta name=twitter:card content="summary"><meta name=twitter:title content="GitLab MCP Server tools 功能扩展实战"><meta name=twitter:description content="使用 typescript 扩展 GitLab MCP Server tools"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>小马哥</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/cloud-native><li class=mobile-menu-item>Cloud Native</li></a><a href=/devsecops><li class=mobile-menu-item>DevSecOps</li></a><a href=/aigc><li class=mobile-menu-item>AIGC</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><head><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','G-T0B2C6W2ZH','auto');ga('set','anonymizeIp',true);ga('send','pageview');}</script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','G-T0B2C6W2ZH','auto');ga('set','anonymizeIp',true);ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><div class=logo-wrapper><a href=/ class=logo>小马哥</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/cloud-native>Cloud Native</a></li><li class=menu-item><a class=menu-item-link href=/devsecops>DevSecOps</a></li><li class=menu-item><a class=menu-item-link href=/aigc>AIGC</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>GitLab MCP Server tools 功能扩展实战</h1><div class=post-meta><span class=post-time>2025-05-01</span><div class=post-category><a href=/categories/aigc/>AIGC</a></div><span class=more-meta>约 2093 字</span>
<span class=more-meta>预计阅读 5 分钟</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><ul><li><a href=#llm-工作原理解释>LLM 工作原理解释</a><ul><li><a href=#条件概率解释>条件概率解释</a></li><li><a href=#llm-预测解释>LLM 预测解释</a></li></ul></li><li><a href=#temperature温度>temperature（温度）</a></li></ul></li></ul></nav></div></div><div class=post-content><p>最近在 x 上看到 Akshay 分享的一组图解 LLM 工作原理的帖子，感觉内容通俗易懂，就搬运过来汉化一下，方便大家一起学习！</p><p>Akshay 是一位 AI/ML 工程师，他在 x 上的介绍如下图所示：</p><p><img src=images/akshay.png alt=akshay></p><h2 id=llm-工作原理解释>LLM 工作原理解释</h2><h3 id=条件概率解释>条件概率解释</h3><p>他提到，在介绍 LLM 之前，需要先了解一下<strong>条件概率</strong>（conditional probability），应该是与高中、大学学的概率学相关。有一个很形象的例子：</p><p>有 14 个人，他们中的一部分人（7 个）喜欢网球、一部分人（8个）喜欢足球、少部分人（3 个）同时喜欢网球和足球、也有极少一部分人（2 个）都不喜欢网球和足球。用图表示如下：</p><p><img src=images/conditional-probability-1.jpeg alt="conditional probability"></p><p>所以如果要表示喜欢网球的人数概率，表示方法为 P(A)，结果是 7/14；喜欢足球的人数概率，表示方法为 P(B)，结果为 8/14；同时喜欢网球和足球的人数概率，表示方法为 P(A∩B)，结果是 3/14；同时表示既不喜欢网球又不喜欢足球的人数概率，表示方法为 P(AUB)，结果为 2/14。</p><p>那什么条件概率呢？</p><p>其实就是在另外一件事情发生的前提下，某件事情发生的概率。比如上面的事件 A 和事件 B，如果要表示在事件 B 发生的前提下，事件 A 发生的概率，那么表示方法是P(A∣B)。</p><p>所以，如果要计算一个人在喜欢足球的情况下，还喜欢网球的概率，计算方法为 P(A|B)=P(A∩B)/P(B)=(3/14)/(8/14)=3/8。</p><p><img src=images/conditional-probability-2.jpeg alt="conditional probability 2"></p><p>再拿阴天和下雨天为例来将条件概率：如果将今天下雨当作事件 A，阴天可能下雨作为事件 B（尝试来讲，阴天会有下雨的可能），而且事件 B 会影响下雨的预测。所以，阴天的时候就可能会下雨，这个时候就可以说条件概率 P(A|B) 是非常高的。</p><h3 id=llm-预测解释>LLM 预测解释</h3><p>回到 LLM 上来说，这些模式的任务就是预测下一个出现的单词。这就和前面讲的条件概率类似：<strong>如果给定已经出现过的单词，那下一个最可能出现的单词是哪一个？</strong></p><p><img src=images/conditional-probability-3.jpeg alt="conditional probability 3"></p><p>所以，要预测下一个单词，<strong>模型就要根据之前给定的单词（上下文）来为每一个接下来可能出现的单词进行条件概率的计算，条件概率最高的单词就会被作为预测单词所选中</strong>。</p><p><img src=images/conditional-probability-4.jpeg alt="conditional probability 4"></p><p>而 LLM 学习的是一个高维度的单词序列概率分布。这个分布的参数就是经过训练的权重。但是这种概率毕竟是一种预测，并不是实际的结果，所以这个过程中就有一个 <strong>损失计算(Loss calculation)</strong> 的概念。</p><p><img src=images/conditional-probability-5.jpeg alt="conditional probability 5"></p><blockquote><p>以下内容来自 ChatGPT。</p></blockquote><p>Loss calculation（损失计算） 是指模型在预测过程中产生的误差的度量，通常用来衡量模型预测的结果与实际目标之间的差异。通过最小化损失函数，模型能够不断优化其参数，以提高对新数据的预测能力。</p><p>上图中提到的 Cross-entropy loss 和 Negative log-likehood 是两种损失函数。</p><ul><li>Cross-entropy loss</li></ul><p>Cross-entropy loss 指交叉墒损失，用来度量模型预测的概率分布与真实标签（即实际单词）间的差异。</p><p><strong>交叉墒</strong> 用于计算两个概率分布之间的差异。在语言模型中，一个概率分布是模型对每个可能的下一个单词的预测概率，另一个是实际的单词标签的“真实分布”（通常是一个one-hot分布，即正确单词的概率为1，其他为0）。交叉熵损失的计算公式如下：</p><p><img src=images/jiaochashang-loss.png alt=jiaochashangloss></p><ul><li>Negative Log-Likelihood</li></ul><p>Negative Log-Likelihood，负对数似然，简称 NLL。是机器学习中常用的一个损失函数，尤其在 概率模型 和 分类问题 中广泛应用。</p><blockquote><p>以上内容来自 ChatGPT。</p></blockquote><p>这种概率预测并选择最有可能的单词会带来一个问题<strong>如果总是选择可能性最大的单词，那么结果就是重复性的，这就让 LLM 显得缺乏创造性</strong>。</p><p>所以，这里面就有一个 **temperature（温度）**的概念产生。</p><h2 id=temperature温度>temperature（温度）</h2><p>LLM 中，temperature（温度）是一个调整模型输出概率分布的超参数，通常用于文本生成和采样。它影响生成文本时的<strong>多样性</strong>和<strong>创造性</strong>，以及模型在选择下一个单词时的随机性。</p><p>因为在 LLM 中，大模型通常会生成一个概率分布，表示下一词在给定上下文下出现的可能性。例如，模型可能会为每个可能的下一个单词生成一个概率，就像前面图中所画的：</p><p>上下文是“The boy went to the“，下一个单词可能是“Cafe、Hospital、Playground、Park、School“，这几个单独对应的概率是“0.1、0.05、0.4、0.15、0.3“。</p><p>temperature 控制如何从概率分布进行采样：</p><ul><li><p>Low temperature（低温度，比如 0.1 ~ 0.5）：模型的输出会更加具有确定性，也就是更倾向于选择概率较高的单词，此时生成的文本更连贯、理性，内容更“保守”，但是也可能缺乏多样性和创意。</p></li><li><p>High temperature（高温度，比如 0.8 ~ 1.0）：模型的输出会更加随机，也就使得低概率的单词有更大的机会被选中。这会增加生成文本的多样性和创造性，但是也可能导致输出不那么流畅或不太符合上下文。</p></li></ul><p>temperature 是通过使用 softmax 函数来调整每个词的 logits（即原始的未经过归一化的分数）进行的。</p><blockquote><p>softmax 函数是一个激活函数，用来讲向量中的每个值转换成一个概率分布。其输出的每个值都会被转换成一个介于 0 和 1 之间的概率，并且所有输出的概率之和等于 1。</p></blockquote><p>随后，作者给了两个不同 temperature 时候的示例来说明差别，第一张图是 low temperature 的，第二张图是 high temperature 的。</p><p><img src=images/low-temperature.jpeg alt="low temperature"></p><p><img src=images/high-temperature.jpeg alt="high temperature"></p><p>所以 LLM 并不是选择最佳（概率最大）的 token，而是对预测进行采样。所以，概率最高的 token 也有可能不会被选中。</p><p><img src=images/sample.jpeg alt=sample></p><p>所以，在 softmax 函数中，温度引入了一些调整，反过来这种调整又影响了采样过程。</p><p><img src=images/softmax-logits.jpeg alt="softmax logits"></p><p>最后作者给了一个很直观的代码示例来对 temperature 对采样的影响：</p><p><img src=images/temperature-code-demo.jpeg alt="code demo"></p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>马景贺（小马哥）</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2025-05-01</span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/ai/>AI</a>
<a href=/tags/mcp/>MCP</a>
<a href=/tags/gitlab/>GitLab</a>
<a href=/tags/typescript/>TypeScript</a></div><nav class=post-nav><a class=next href=/aigc/mcp-gitlab/><span class="next-text nav-default">GitLab MCP Server tools 功能扩展实战</span>
<span class="next-text nav-mobile">下一篇</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=social-links><a href=devops008@sina.com class="iconfont icon-email" title=email></a><a href=https://github.com/majinghe class="iconfont icon-github" title=github></a><a href=https://majinghe.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>由 <a class=hexo-link href=https://gohugo.io>Hugo</a> 强力驱动</span>
<span class=division>|</span>
<span class=theme-info>主题 -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span>
<span class=copyright-year>&copy;
2020 -
2025<span class=heart><i class="iconfont icon-heart"></i></span><span>olOwOlo</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','G-T0B2C6W2ZH','auto');ga('set','anonymizeIp',true);ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></body></html>